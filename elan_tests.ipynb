{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niko Partanen, 3.12.2019\n",
    "\n",
    "## ELAN tests\n",
    "\n",
    "These are the ELAN file validation tests used in IKDP research project, and it's continuation project [Language Documentation meets Language Technology: The Next Step in the Description of Komi](https://langdoc.github.io/IKDP-2/). Some parts of the code is very old, some is never, and with some parts the final implementation is still being thought about. Especially with tests that interact between ELAN files and metadata there are countless ways to do them effectively, and the current method is probably not the final. Similarly, it is still bit unclear what is the best way to store project's common attributes. Currently we are using a YAML file called `project.yaml`, but there are maybe other better alternatives to that. The idea is that in principle the methods could be adapted into other projects by editing this configuration file. In practice this may be more complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use a lot Pympi package in these tests, but I did make a change in `Elan.py` file at line 1437. Otherwise there are warnings everytime I parse an ELAN file. So this:\n",
    "\n",
    "```\n",
    "if tree_root.attrib['VERSION'] not in ['2.8', '2.7']:\n",
    "```\n",
    "\n",
    "Is changed to:\n",
    "\n",
    "```\n",
    "if tree_root.attrib['VERSION'] not in ['2.8', '2.7', '3.0']:\n",
    "```\n",
    "\n",
    "Mikatools contains some functions used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mikatools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tests themselves are loaded from this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elan_tests import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are other packages that are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uralicNLP import uralicApi\n",
    "from uralicNLP.cg3 import Cg3\n",
    "import pympi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What it comes to corpus metadata, the scripts are currently assuming following structure in a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_json = []\n",
    "\n",
    "session = {}\n",
    "\n",
    "session[\"session_name\"] = \"recording_session_1\"\n",
    "\n",
    "participants = [{\"participant\":\"S1\"},\n",
    "                {\"participant\":\"S2\"}]\n",
    "\n",
    "session[\"participants\"] = participants\n",
    "\n",
    "meta_json.append(session)\n",
    "\n",
    "meta_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if the ELAN file is called `recording_session_1.eaf`, then in metadata the information about this recording is stored under object with this identifier. Under each session there is participant information, and there the field `participant` has the id's that are present in the ELAN files as well.\n",
    "\n",
    "In this point we are mainly testing for two potential problems. First, every ELAN file should have corresponding item in the metadata. If the ELAN file's session name is not matching with anything in metadata, then it will be impossible to match any external information about recording to the content of ELAN file. Second, each participant in ELAN file should be present in the right place within the metadata. It is of course entirely possible that in the metadata there are participants who are not present in the metadata, for example, if someone didn't say anything during the recording, but was present anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project's own metadata is easily loaded this way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_meta = json_load('../ikdp-meta/ikdp_meta.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the tests one by one\n",
    "\n",
    "The easiest way to run the tests is to set them into a loop that goes through each ELAN file, and prints out the result if there are some issues. \n",
    "\n",
    "Some tests are possibly only if previous tests succeeded. I.e. it is useless to try finding a speaker ID from metadata if the session is not found. There is probably quite much logic that could be set into the steps better, but at least the current structure attempts to reflect movement from one domain to another in the order of increasing complexity. \n",
    "\n",
    "Our focus is in structural issues that would potentially invalidate the ELAN file. And these tests either focus into issues in ELAN files or in ways information in ELAN files and metadata may mismatch in harmful way. One could argue, that things like whether different metadata attributes are present could also be checked. Yes, they could and we should do it, but if information is not somehow repeated in ELAN files, the problem their lacking or changing values pose is not directly connected to ELAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elan_file_paths = glob.glob(f\"{corpus_location}/**/kpv_izva*.eaf\", recursive=True)\n",
    "\n",
    "for elan_file in elan_file_paths:\n",
    "    \n",
    "    pass\n",
    "    \n",
    "#   Is ELAN file named according to the scheme\n",
    "#   test_session_names(elan_file)\n",
    "    \n",
    "#    In transcriptions, do we have only characters that are supposed to be there\n",
    "#    check_illegal_characters(elan_file, verification_list = manually_verified_files)\n",
    "\n",
    "#    Do we have other whitespace than spaces\n",
    "#    check_illegal_characters(elan_file, verification_list = manually_verified_files, tier_type = \"orthT\", unwanted_characters = \"[\\t\\n\\r]\")\n",
    "\n",
    "#    Are all required tier types present\n",
    "#    test_tier_types(elan_file, types = ['refT', 'orthT', 'ft-engT', 'ft-rusT'])\n",
    "\n",
    "#    Are all wanted tiers present in the file\n",
    "#    test_tier_existence(elan_file, tier_prefixes = ['ref', 'orth', 'ft-word', 'ft-rus'])\n",
    "\n",
    "#    Are right tier types used on right tiers\n",
    "#    test_tier_type_consistency(elan_file)\n",
    "\n",
    "#    # Is session name in metadata\n",
    "#    test_tier_type_consistency(elan_file)\n",
    "\n",
    "#    Is participant ID in metadata\n",
    "#    test_participant_meta(elan_file, corpus_meta, \"orthT\")\n",
    "\n",
    "### NOT YET IMPLEMENTED\n",
    "\n",
    "#    Check if linked files exist\n",
    "#    check_linked_files(elan_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to go one step further, and analyse the linguistic content directly. For example, as we have a Komi morphological analyser, we can test which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пасьтадас (2)\n",
      "ӧддьйӧджык (1)\n",
      "чысьянӧдас (1)\n",
      "сійӧс-тӧ (1)\n",
      "пышйас (1)\n",
      "платтьӧн (1)\n",
      "ней (1)\n",
      "мӧдӧдіс. (1)\n",
      "лэдзы (1)\n",
      "ичӧтьлик (1)\n",
      "гӧсьньӧч (1)\n",
      "гырйас (1)\n",
      "вӧртіын (1)\n",
      "верднылӧй (1)\n",
      "бӧртчис (1)\n",
      "аслы (1)\n",
      "Патурлик (1)\n",
      "Всё (1)\n"
     ]
    }
   ],
   "source": [
    "print_unknown_words(\"/Volumes/langdoc/langs/kpv/kpv_lit19570000lytkin-1323_2az/kpv_lit19570000lytkin-1323_2az.eaf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, there are dialectal words, unknown words and Russian words that do not get an analysis. Also there are incorrectly tokenized words, which brings us to other questions, such as how to best way tokenize a language documentation corpus etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:elan-tests]",
   "language": "python",
   "name": "conda-env-elan-tests-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
